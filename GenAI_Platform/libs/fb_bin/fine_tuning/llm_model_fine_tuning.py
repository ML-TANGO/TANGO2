import argparse
import shutil
import os
import glob
import traceback
import subprocess
import signal
import json
import pytz
from datetime import datetime
import numpy as np
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments, 
    Trainer, 
    AutoConfig, 
    BitsAndBytesConfig, 
    TrainerCallback, 
    DataCollatorForLanguageModeling
)
from transformers.utils import logging
from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig
from datasets import load_dataset, Dataset, load_from_disk
import torch
import torch.nn as nn
from db import update_steps_per_epoch, update_finetuning_error_status, update_finetuning_running_status
from typing import Optional, Dict, Any, Tuple, Union
from dataclasses import dataclass
from pathlib import Path


@dataclass
class TrainingConfig:
    """í•™ìŠµ ì„¤ì •ì„ ê´€ë¦¬í•˜ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    fp16: bool = True  # 16ë¹„íŠ¸ ì •ë°€ë„ ì‚¬ìš©ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½ ë° í•™ìŠµ ì†ë„ í–¥ìƒ
    weight_decay: float = 0.01  # ê°€ì¤‘ì¹˜ ê°ì‡ ë¡œ ê³¼ì í•© ë°©ì§€
    overwrite_output_dir: bool = True  # ê¸°ì¡´ ì¶œë ¥ ë””ë ‰í† ë¦¬ ë®ì–´ì“°ê¸°
    output_dir: str = "/tmp_model"  # ëª¨ë¸ ì €ì¥ ê²½ë¡œ
    logging_dir: str = "/logs"  # ë¡œê·¸ ì €ì¥ ê²½ë¡œ
    remove_unused_columns: bool = True  # ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì»¬ëŸ¼ ì œê±°ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
    save_total_limit: int = 1  # ì €ì¥í•  ì²´í¬í¬ì¸íŠ¸ ìˆ˜ ì œí•œ
    save_strategy: str = "steps"  # ìŠ¤í… ë‹¨ìœ„ë¡œ ëª¨ë¸ ì €ì¥
    logging_strategy: str = "steps"  # ìŠ¤í… ë‹¨ìœ„ë¡œ ë¡œê·¸ ê¸°ë¡
    eval_strategy: str = "steps"  # ìŠ¤í… ë‹¨ìœ„ë¡œ í‰ê°€ ì‹¤í–‰
    eval_steps: int = 5  # í‰ê°€ ì‹¤í–‰ ê°„ê²©
    logging_steps: int = 5 # ë¡œê·¸ ê¸°ë¡ ê°„ê²©
    save_steps: int = 1000  # ëª¨ë¸ ì €ì¥ ê°„ê²©
    prediction_loss_only: bool = True  # ì˜ˆì¸¡ ì†ì‹¤ë§Œ ê³„ì‚°í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
    dataloader_pin_memory: bool = False  # ë©”ëª¨ë¦¬ ê³ ì • ë¹„í™œì„±í™” (DeepSpeed í˜¸í™˜ì„±)
    dataloader_num_workers: int = 0  # ë°ì´í„° ë¡œë” ì›Œì»¤ ìˆ˜ (DeepSpeed í˜¸í™˜ì„±)


@dataclass
class FineTuningArgs:
    """íŒŒì¸íŠœë‹ ì¸ìë“¤ì„ ê´€ë¦¬í•˜ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    local_rank: int  # ë¶„ì‚° í•™ìŠµì—ì„œ í˜„ì¬ í”„ë¡œì„¸ìŠ¤ì˜ ë¡œì»¬ ë­í¬
    num_nodes: int  # ë¶„ì‚° í•™ìŠµì— ì‚¬ìš©í•  ë…¸ë“œ ìˆ˜
    num_gpus: int  # ê° ë…¸ë“œì—ì„œ ì‚¬ìš©í•  GPU ìˆ˜
    dataset_path: str  # í•™ìŠµ ë°ì´í„°ì…‹ ê²½ë¡œ
    config_path: Optional[str] = None  # ì‚¬ìš©ì ì •ì˜ ì„¤ì • íŒŒì¼ ê²½ë¡œ
    num_train_epochs: float = 3.0  # ì „ì²´ í•™ìŠµ ì—í¬í¬ ìˆ˜
    gradient_accumulation_steps: int = 1  # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í… ìˆ˜ (ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì‹œë®¬ë ˆì´ì…˜)
    cutoff_length: int = 128  # ì…ë ¥ ì‹œí€€ìŠ¤ ìµœëŒ€ í† í° ê¸¸ì´
    learning_rate: float = 5e-5  # í•™ìŠµë¥ 
    warmup_steps: int = 0  # í•™ìŠµë¥  ì›Œë°ì—… ìŠ¤í… ìˆ˜
    used_lora: int = 0  # LoRA ì‚¬ìš© ì—¬ë¶€ (0: ë¹„í™œì„±í™”, 1: í™œì„±í™”)
    used_dist: int = 1  # ë¶„ì‚° í•™ìŠµ ì‚¬ìš© ì—¬ë¶€
    load_in_8bit: int = 0  # 8ë¹„íŠ¸ ì–‘ìí™” ì‚¬ìš© ì—¬ë¶€
    deepspeed: Optional[str] = None  # DeepSpeed ì„¤ì • íŒŒì¼ ê²½ë¡œ


class PathManager:
    """ê²½ë¡œ ê´€ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.model_path = Path("/model")  # ê¸°ë³¸ ëª¨ë¸ ê²½ë¡œ
        self.lora_source_model_path = Path("/model/source_model")  # LoRA ì†ŒìŠ¤ ëª¨ë¸ ê²½ë¡œ
        self.log_path = Path("/logs")  # ë¡œê·¸ ì €ì¥ ê²½ë¡œ
        self.output_path = Path("/tmp_model")  # ì„ì‹œ ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        self.config_path = Path("/configurations")  # ì„¤ì • íŒŒì¼ ê²½ë¡œ
        self.fine_tuning_config_path = Path("/fine_tuning/ds_config.json")  # DeepSpeed ì„¤ì • ê²½ë¡œ
    
    def get_model_path(self) -> Path:
        """ì‚¬ìš©í•  ëª¨ë¸ ê²½ë¡œë¥¼ ë°˜í™˜ - LoRA ì†ŒìŠ¤ ëª¨ë¸ì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©"""
        return self.lora_source_model_path if self.lora_source_model_path.exists() else self.model_path
    
    def ensure_directories(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ë“¤ì„ ìƒì„±"""
        for path in [self.output_path, self.log_path]:
            path.mkdir(parents=True, exist_ok=True)


class SeoulTimeFormatter(logging.logging.Formatter):
    """ì„œìš¸ ì‹œê°„ëŒ€ë¥¼ ì‚¬ìš©í•˜ëŠ” ë¡œê·¸ í¬ë§¤í„°"""
    
    def __init__(self, fmt=None, datefmt=None, style='%'):
        super().__init__(fmt, datefmt, style)
        self.seoul_tz = pytz.timezone("Asia/Seoul")

    def formatTime(self, record, datefmt=None):
        utc_time = datetime.utcfromtimestamp(record.created)
        local_time = utc_time.astimezone(self.seoul_tz)
        if datefmt:
            return local_time.strftime(datefmt)
        return local_time.isoformat()


class LoggerSetup:
    """ë¡œê±° ì„¤ì •ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    @staticmethod
    def setup():
        logging.set_verbosity_info()
        logger = logging.get_logger("transformers")
        logger.setLevel(logging.INFO)

        console_handler = logging.logging.StreamHandler()
        formatter = SeoulTimeFormatter(
            "%(asctime)s - %(levelname)s - %(message)s", 
            "%Y-%m-%d %H:%M:%S"
        )
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)
        
        return logger


class SavePeftAdapterCallback(TrainerCallback):
    """LoRA ì–´ëŒ‘í„° ì €ì¥ì„ ìœ„í•œ ì½œë°± - ì²´í¬í¬ì¸íŠ¸ë§ˆë‹¤ ì–´ëŒ‘í„° ê°€ì¤‘ì¹˜ ì €ì¥"""
    
    def __init__(self, save_directory: str):
        self.save_directory = save_directory
        self.logger = logging.get_logger("transformers")

    def on_save(self, args, state, control, **kwargs):
        model = kwargs['model']
        
        if isinstance(model, PeftModel):
            adapter_save_path = os.path.join(self.save_directory, f"checkpoint-{state.global_step}")
            os.makedirs(adapter_save_path, exist_ok=True)
            model.save_pretrained(adapter_save_path)
            self.logger.info(f"Adapter saved at {adapter_save_path}")


class GPUManager:
    """GPU ê´€ë ¨ ê¸°ëŠ¥ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    @staticmethod
    def get_gpu_vram(device) -> float:
        """GPU VRAM í¬ê¸°ë¥¼ GB ë‹¨ìœ„ë¡œ ë°˜í™˜"""
        vram_bytes = torch.cuda.get_device_properties(device).total_memory
        return vram_bytes / (1024 ** 3)

    @staticmethod
    def get_optimal_batch_size(vram_gb: float) -> int:
        """VRAM ìš©ëŸ‰ì— ë”°ë¼ ìµœì ì˜ ë°°ì¹˜ í¬ê¸°ë¥¼ ë°˜í™˜"""
        if vram_gb <= 4:
            return 1
        elif vram_gb <= 8:
            return 2
        elif vram_gb <= 16:
            return 4
        elif vram_gb <= 24:
            return 8
        else:
            return 16


class FileManager:
    """íŒŒì¼ ë° ë””ë ‰í† ë¦¬ ê´€ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤"""
    
    logger = logging.get_logger("transformers")

    @staticmethod
    def clear_dir_contents(path: Path):
        """ë””ë ‰í† ë¦¬ ë‚´ìš©ì„ ëª¨ë‘ ì‚­ì œ"""
        if not path.is_dir():
            FileManager.logger.warning(f"ê²½ë¡œ {path}ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return

        for item in path.iterdir():
            try:
                if item.is_dir():
                    shutil.rmtree(item)
                else:
                    item.unlink()
            except Exception as e:
                FileManager.logger.error(f"{item} ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    @staticmethod
    def delete_checkpoint_dir(path: Path):
        """ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ë“¤ì„ ì‚­ì œ - ìµœì¢… ëª¨ë¸ë§Œ ìœ ì§€"""
        if not path.is_dir():
            FileManager.logger.warning(f"ê²½ë¡œ {path}ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return

        deleted_folders = []
        for item in path.iterdir():
            if item.is_dir() and item.name.startswith("checkpoint-"):
                shutil.rmtree(item)
                deleted_folders.append(str(item))

        if deleted_folders:
            FileManager.logger.info(f"ì‚­ì œëœ í´ë”: {deleted_folders}")
        else:
            FileManager.logger.info("ì‚­ì œí•  'checkpoint-'ë¡œ ì‹œì‘í•˜ëŠ” í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")

    @staticmethod
    def copy_dir_contents(src_folder: Path, dest_folder: Path):
        """src_folderì˜ ë‚´ìš©ë§Œ dest_folderë¡œ ë³µì‚¬"""
        dest_folder.mkdir(parents=True, exist_ok=True)
        
        try:
            subprocess.run(['cp', '-r', f'{src_folder}/.', str(dest_folder)], check=True)
        except subprocess.CalledProcessError as e:
            FileManager.logger.error(f"Error copying directory: {e}")


class ModelManager:
    """ëª¨ë¸ ê´€ë ¨ ê¸°ëŠ¥ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""

    @staticmethod
    def get_target_modules(model) -> list:
        """ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ LoRA íƒ€ê²Ÿ ëª¨ë“ˆì„ ë°˜í™˜ - attention ê´€ë ¨ ëª¨ë“ˆë“¤ì— LoRA ì ìš©"""
        model_name = model.__class__.__name__.lower()

        # 1) ê° ëª¨ë¸ ì•„í‚¤í…ì²˜ë³„ LoRA ì ìš©í•  attention ëª¨ë“ˆ ë§¤í•‘
        target_modules_map = {
            "gpt2":       ["c_attn", "c_proj"],                  # GPT-2ëŠ” Q/K/Vë¥¼ í•©ì¹œ c_attnê³¼ ì¶œë ¥ íˆ¬ì˜ c_proj ì‚¬ìš© :contentReference[oaicite:0]{index=0}
            "gptj":       ["c_attn", "c_proj"],                  # GPT-Jë„ GPT-2 ìŠ¤íƒ€ì¼ Conv1D ì‚¬ìš© :contentReference[oaicite:1]{index=1}
            "gpt_neo":    ["c_attn", "c_proj"],                  # GPT-Neo ì—­ì‹œ c_attn, c_proj ë„¤ì´ë° ì‚¬ìš© :contentReference[oaicite:2]{index=2}
            "gpt_neox":   ["query_key_value", "dense"],          # GPT-NeoXëŠ” query_key_value í•©ì‚° íˆ¬ì˜ê³¼ dense ì¶œë ¥ íˆ¬ì˜ :contentReference[oaicite:3]{index=3}
            "opt":        ["q_proj", "k_proj", "v_proj", "out_proj"],  # OPTAttentionì˜ q_proj/k_proj/v_proj/out_proj :contentReference[oaicite:4]{index=4}
            "bloom":      ["self_attention.query_key_value", "self_attention.dense"],  # BLOOMì€ QKV í•©ì‚°, dense íˆ¬ì˜ 
            "llama":      ["q_proj", "k_proj", "v_proj", "o_proj"],       # LLaMA self-attention 
            "llama2":     ["q_proj", "k_proj", "v_proj", "o_proj"],       # LLaMA-2ë„ ë™ì¼ ë„¤ì´ë° :contentReference[oaicite:7]{index=7}
            "qwen":       ["q_proj", "k_proj", "v_proj", "o_proj"],       # Qwenì€ LLaMA ê³„ì—´ ê¸°ë°˜ 
            "deepseek-r1":["q_proj", "k_proj", "v_proj", "o_proj"],       # DeepSeek-R1 self-attention :contentReference[oaicite:9]{index=9}
            "mpt":        ["q_proj", "k_proj", "v_proj", "o_proj"],       # MPTëŠ” q_proj/k_proj/v_proj/out_proj ì‚¬ìš© :contentReference[oaicite:10]{index=10}
            "falcon":     ["q_proj", "k_proj", "v_proj", "out_proj"],     # Falconë„ Transformer ê¸°ë°˜ q_proj/k_proj/v_proj/out_proj :contentReference[oaicite:11]{index=11}
        }

        # 2) í‚¤ì›Œë“œ ë§¤í•‘ ìš°ì„  ì ìš©
        for keyword, modules in target_modules_map.items():
            if keyword in model_name:
                base_modules = modules.copy()
                break
        else:
            # 3) Mistral/Mixtral íŠ¹ìˆ˜ ì²˜ë¦¬
            if any(k in model_name for k in ["mistral", "mixtral"]):
                base_modules = ["q_proj", "v_proj"]
            else:
                # 4) ê¸°ë³¸ ìë™ íƒì§€ (ì´ë¦„ì— attn/attention/proj ê°€ ë“¤ì–´ê°€ëŠ” ëª¨ë“  ëª¨ë“ˆ)
                base_modules = [
                    name for name, _ in model.named_modules()
                    if any(tag in name.lower() for tag in ["attn", "attention", "proj"])
                ]

        # 5) nn.Linear ë ˆì´ì–´ ìŠ¤ìº”ì„ í†µí•´ ë¹ ì§„ attention ê³„ì—´ ë ˆì´ì–´ëª… ë³´ì™„
        #    â€” ì´ë¦„ì— proj/attn/query/key/value ë“±ì´ í¬í•¨ëœ nn.Linearë¥¼ ì¶”ê°€
        all_modules = set(base_modules)
        for name, module in model.named_modules():
            if isinstance(module, nn.Linear):
                lname = name.lower()
                if any(tag in lname for tag in ["proj", "attn", "query", "key", "value"]):
                    all_modules.add(name)

        # 6) ìµœì¢… ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        #    â€” ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ set â†’ list ë³€í™˜, ì •ë ¬(Optional)
        return sorted(all_modules)


class DatasetManager:
    """ë°ì´í„°ì…‹ ê´€ë ¨ ê¸°ëŠ¥ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    logger = logging.get_logger("transformers")

    @staticmethod
    def print_dataset_info(dataset):
        """ë°ì´í„°ì…‹ ì •ë³´ë¥¼ ì¶œë ¥"""
        for split in dataset.keys():
            DatasetManager.logger.info(f"Split: {split}, Number of examples: {len(dataset[split])}")
            DatasetManager.logger.info(f"Columns: {dataset[split].column_names}")

    @staticmethod
    def load_dataset_by_path(dataset_path: str, split: str = None) -> Union[Dataset, Dict[str, Dataset]]:
        """í™•ì¥ì ë˜ëŠ” ë””ë ‰í† ë¦¬ì— ë”°ë¼ ìë™ìœ¼ë¡œ ë°ì´í„°ì…‹ ë¡œë“œ"""
        path = Path(dataset_path)
        
        if not path.exists():
            raise FileNotFoundError(f"âŒ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {dataset_path}")

        if path.is_dir():
            print(f"[INFO] ë””ë ‰í† ë¦¬ ê¸°ë°˜ arrow ë¡œë“œ: {dataset_path}")
            return load_from_disk(dataset_path)

        ext = path.suffix.lower()
        loader_args = {"data_files": dataset_path}
        if split:
            loader_args["split"] = split

        if ext in [".json", ".jsonl"]:
            print(f"[INFO] JSON íŒŒì¼ ë¡œë“œ: {dataset_path}")
            return load_dataset("json", **loader_args)
        elif ext == ".csv":
            print(f"[INFO] CSV íŒŒì¼ ë¡œë“œ: {dataset_path}")
            return load_dataset("csv", **loader_args)
        elif ext == ".arrow":
            print(f"[INFO] Arrow íŒŒì¼ ë¡œë“œ: {dataset_path}")
            return Dataset.from_file(dataset_path)
        else:
            raise ValueError(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” í™•ì¥ì: {ext}")

    @staticmethod
    def prepare_datasets(dataset_path: str, cutoff_length: int, tokenizer) -> Tuple[Dataset, Optional[Dataset]]:
        """ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬ - í† í¬ë‚˜ì´ì§• ë° train/validation ë¶„í• """
        dataset = DatasetManager.load_dataset_by_path(dataset_path=dataset_path)
        
        # Arrow íŒŒì¼ ë“±ì—ì„œ ë‹¨ì¼ Datasetì´ ë°˜í™˜ëœ ê²½ìš° ì²˜ë¦¬
        if isinstance(dataset, Dataset):
            print(f"[INFO] ë‹¨ì¼ Dataset ê°ì§€. ì „ì²´ ë°ì´í„°ë¥¼ trainìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            # ì „ì²´ ë°ì´í„°ë¥¼ trainìœ¼ë¡œ ì‚¬ìš© (evaluation ì—†ì´ í•™ìŠµ)
            train_dataset = dataset
            eval_dataset = None
            print(f"ğŸ“ˆ Train ìƒ˜í”Œ ìˆ˜: {len(train_dataset)}")
            print(f"ğŸ“‰ Evaluation ë°ì´í„°ì…‹ ì—†ì´ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.")
        else:
            # DatasetDictì¸ ê²½ìš° ê¸°ì¡´ ë¡œì§ ì‚¬ìš©
            # train/validation ë¶„í•  - validationì´ ìˆìœ¼ë©´ validation ì‚¬ìš©, ì—†ìœ¼ë©´ test ì‚¬ìš©
            train_dataset = dataset['train']
            eval_dataset = dataset.get('validation', dataset.get('test'))
            
            # ë°ì´í„°ì…‹ì— ìˆëŠ” split ì¢…ë¥˜ ë¡œê·¸ ì¶œë ¥
            available_splits = list(dataset.keys())
            print(f"ğŸ“Š ë°ì´í„°ì…‹ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ split ì¢…ë¥˜: {available_splits}")
            print(f"ğŸ“ˆ ì‚¬ìš©í•  train split: train (ìƒ˜í”Œ ìˆ˜: {len(train_dataset)})")
            
            if eval_dataset is not None:
                eval_split_name = 'validation' if 'validation' in dataset else 'test'
                print(f"ğŸ“‰ ì‚¬ìš©í•  evaluation split: {eval_split_name} (ìƒ˜í”Œ ìˆ˜: {len(eval_dataset)})")
            else:
                print(f"âš ï¸ ê²½ê³ : ë°ì´í„°ì…‹ì— 'validation' ë˜ëŠ” 'test' ìŠ¤í”Œë¦¿ì´ ì—†ìŠµë‹ˆë‹¤. "
                      f"í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ìŠ¤í”Œë¦¿: {available_splits}. "
                      f"í‰ê°€ ë°ì´í„°ì…‹ ì—†ì´ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.")

        # í† í¬ë‚˜ì´ì§• í•¨ìˆ˜ - í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜í•˜ê³  labels ì„¤ì •
        def tokenize_function(examples):
            tok = tokenizer(
                examples["text"],
                padding="max_length",   # í•­ìƒ max_length ê¸¸ì´ë¡œ íŒ¨ë”©
                truncation=True,
                max_length=cutoff_length,
                return_attention_mask=True
                # examples['text'],
                # padding=True,
                # max_length=cutoff_length,
                # truncation=True,
                # return_tensors='pt',
                # return_attention_mask=True
            )
            # causal LMìš© labels = input_ids ë³µì‚¬ë³¸
            tok["labels"] = [list(ids) for ids in tok["input_ids"]]
            return tok

        # ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì§• ì ìš©
        tokenized_train_dataset = train_dataset.map(
            tokenize_function, 
            batched=True, 
            keep_in_memory=True
        )
        
        # eval_datasetì´ ìˆëŠ” ê²½ìš°ì—ë§Œ í† í¬ë‚˜ì´ì§• ì ìš©
        tokenized_eval_dataset = None
        if eval_dataset is not None:
            tokenized_eval_dataset = eval_dataset.map(
                tokenize_function, 
                batched=True, 
                keep_in_memory=True
            )
            # ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ í¬ë§· ì„¤ì • - PyTorch í…ì„œ í˜•íƒœë¡œ ë³€í™˜
            tokenized_eval_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])

        # ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ í¬ë§· ì„¤ì • - PyTorch í…ì„œ í˜•íƒœë¡œ ë³€í™˜
        tokenized_train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])

        return tokenized_train_dataset, tokenized_eval_dataset


class TrainingManager:
    """í•™ìŠµ ê´€ë ¨ ê¸°ëŠ¥ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, logger):
        self.logger = logger
        self.path_manager = PathManager()
        self.training_config = TrainingConfig()
        # TrainingArgumentsì—ì„œ ì œì™¸í•  íŒŒë¼ë¯¸í„°ë“¤ (ì¤‘ë³µ ë°©ì§€)
        self.exclude_keys = {
            "fp16", "weight_decay", "overwrite_output_dir", "output_dir", "logging_dir",
            "remove_unused_columns", "save_total_limit", "save_strategy", "logging_strategy",
            "eval_strategy", "eval_steps", "logging_steps", "save_steps", "prediction_loss_only",
            "dataloader_pin_memory", "dataloader_num_workers"
        }

    def create_training_arguments(self, args: FineTuningArgs, remove_unused_columns: bool) -> TrainingArguments:
        """TrainingArguments ìƒì„± - ì‚¬ìš©ì ì„¤ì • íŒŒì¼ ë˜ëŠ” ê¸°ë³¸ DeepSpeed ì„¤ì • ì‚¬ìš©"""
        self.training_config.remove_unused_columns = remove_unused_columns
        
        if args.config_path:
            # ì‚¬ìš©ì ì •ì˜ ì„¤ì • íŒŒì¼ ë¡œë“œ
            config_path = self.path_manager.config_path / args.config_path
            with open(config_path, "r", encoding="utf-8") as file:
                config = json.load(file)
            
            # cutoff_lengthëŠ” í† í¬ë‚˜ì´ì§•ì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë˜ë¯€ë¡œ ì œê±°
            if config.get("cutoff_length"):
                del config["cutoff_length"]
            
            # ì œì™¸í•  íŒŒë¼ë¯¸í„°ë¥¼ ì œê±°í•˜ê³  í•„í„°ë§í•˜ì—¬ ì¤‘ë³µ ë°©ì§€
            filtered_data = {key: value for key, value in config.items() 
                           if key not in self.exclude_keys}
            
            return TrainingArguments(**filtered_data, **self.training_config.__dict__)
        else:
            # ë””í´íŠ¸ DeepSpeed config ì‚¬ìš©
            return TrainingArguments(
                learning_rate=args.learning_rate,
                gradient_accumulation_steps=args.gradient_accumulation_steps,
                num_train_epochs=args.num_train_epochs,
                warmup_steps=args.warmup_steps,
                deepspeed=str(self.path_manager.fine_tuning_config_path),
                **self.training_config.__dict__
            )

    def calculate_steps_per_epoch(self, training_args: TrainingArguments, train_dataset: Dataset) -> int:
        """epochë‹¹ step ìˆ˜ ê³„ì‚° - ë¶„ì‚° í•™ìŠµê³¼ ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì ì„ ê³ ë ¤í•œ ì‹¤ì œ ë°°ì¹˜ í¬ê¸° ê³„ì‚°"""
        world_size = torch.distributed.get_world_size() if torch.distributed.is_initialized() else 1
        batch_size_per_device = training_args.per_device_train_batch_size
        gradient_accumulation_steps = training_args.gradient_accumulation_steps

        # ì‹¤ì œ ë°°ì¹˜ í¬ê¸° = (ë””ë°”ì´ìŠ¤ë‹¹ ë°°ì¹˜ í¬ê¸°) Ã— (ë””ë°”ì´ìŠ¤ ìˆ˜) Ã— (ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í…)
        effective_batch_size = batch_size_per_device * world_size * gradient_accumulation_steps
        self.logger.info(f"Effective Batch Size with DeepSpeed: {effective_batch_size}")
        
        total_samples = len(train_dataset)
        # epochë‹¹ ìŠ¤í… ìˆ˜ = (ì „ì²´ ìƒ˜í”Œ ìˆ˜) Ã· (ì‹¤ì œ ë°°ì¹˜ í¬ê¸°) + (ë‚˜ë¨¸ì§€ê°€ ìˆìœ¼ë©´ 1)
        steps_per_epoch = (total_samples // effective_batch_size) + (1 if total_samples % effective_batch_size > 0 else 0)
        self.logger.info(f"Steps per Epoch with DeepSpeed: {steps_per_epoch}")
        
        return steps_per_epoch

    def train_model(self, model, train_dataset: Dataset, eval_dataset: Optional[Dataset], 
                   training_args: TrainingArguments, tokenizer, data_collator=None):
        """ëª¨ë¸ í•™ìŠµ ì‹¤í–‰ - Trainerë¥¼ ì‚¬ìš©í•œ ì‹¤ì œ í•™ìŠµ í”„ë¡œì„¸ìŠ¤"""
        # transformers 4.52.4 í˜¸í™˜ì„±ì„ ìœ„í•œ ì„¤ì • - DeepSpeedì™€ì˜ í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°
        training_args.dataloader_pin_memory = False
        training_args.dataloader_num_workers = 0
        
        # eval_datasetì´ Noneì¸ ê²½ìš° í‰ê°€ ê´€ë ¨ ì„¤ì • ë¹„í™œì„±í™”
        if eval_dataset is None:
            training_args.eval_strategy = "no"
            training_args.eval_steps = None
            self.logger.info("âš ï¸ í‰ê°€ ë°ì´í„°ì…‹ì´ ì—†ì–´ í‰ê°€ë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
        
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=tokenizer,
            optimizers=(None, None),  # DeepSpeedê°€ optimizerë¥¼ ê´€ë¦¬í•˜ë¯€ë¡œ None
            data_collator=data_collator,
            callbacks=[SavePeftAdapterCallback(save_directory=training_args.output_dir)]
        )
        
        self.logger.info("TRAINING START")
        trainer.can_return_loss = True  # ì†ì‹¤ ë°˜í™˜ í™œì„±í™”
        trainer.train()
        self.logger.info("TRAINING COMPLETE")
        
        return trainer


class FineTuningEngine:
    """íŒŒì¸íŠœë‹ ì—”ì§„ ë©”ì¸ í´ë˜ìŠ¤ - ì „ì²´ íŒŒì¸íŠœë‹ í”„ë¡œì„¸ìŠ¤ ì¡°ìœ¨"""
    
    def __init__(self):
        self.logger = LoggerSetup.setup()
        self.path_manager = PathManager()
        self.training_manager = TrainingManager(self.logger)
        self.pod_index = int(os.getenv("POD_INDEX", 0))  # ë¶„ì‚° í•™ìŠµì—ì„œ í˜„ì¬ pod ì¸ë±ìŠ¤

    def load_tokenizer(self, model_path: Path):
        """í† í¬ë‚˜ì´ì € ë¡œë“œ - íŒ¨ë”© í† í°ì„ EOS í† í°ìœ¼ë¡œ ì„¤ì •"""
        try:
            tokenizer = AutoTokenizer.from_pretrained(str(model_path))
            tokenizer.pad_token = tokenizer.eos_token  # íŒ¨ë”© í† í°ì„ EOS í† í°ìœ¼ë¡œ ì„¤ì •
            self.logger.info("TOKENIZER LOAD SUCCESS")
            return tokenizer
        except Exception as e:
            traceback.print_exc()
            self.logger.error(f"ëª¨ë¸ í† í° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            update_finetuning_error_status()
            raise e

    def load_model(self, model_path: Path, args: FineTuningArgs) -> Tuple[Any, bool]:
        """ëª¨ë¸ ë¡œë“œ ë° LoRA ì„¤ì • - 8ë¹„íŠ¸ ì–‘ìí™”, LoRA ì ìš© ë“±"""
        try:
            is_source_model_lora = False
            remove_unused_columns = True
            
            # LoRA ì†ŒìŠ¤ ëª¨ë¸ì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
            if self.path_manager.lora_source_model_path.exists():
                model_path = self.path_manager.lora_source_model_path
                is_source_model_lora = True
            else:
                model_path = self.path_manager.model_path

            # ëª¨ë¸ ë¡œë“œ - 8ë¹„íŠ¸ ì–‘ìí™” ì˜µì…˜ ì§€ì›
            if args.load_in_8bit:
                quantization_config = BitsAndBytesConfig(load_in_8bit=True)
                model = AutoModelForCausalLM.from_pretrained(
                    str(model_path),
                    quantization_config=quantization_config
                )
            else:
                model = AutoModelForCausalLM.from_pretrained(str(model_path))

            # LoRA ì„¤ì • - ê¸°ì¡´ LoRA ëª¨ë¸ ë³‘í•© ë˜ëŠ” ìƒˆ LoRA ì ìš©
            if args.used_lora or is_source_model_lora:
                if is_source_model_lora:
                    # ê¸°ì¡´ LoRA ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ë³‘í•©
                    model = PeftModel.from_pretrained(model, str(self.path_manager.model_path))
                    model = model.merge_and_unload()  # LoRA ê°€ì¤‘ì¹˜ë¥¼ ì›ë³¸ ëª¨ë¸ì— ë³‘í•©
                    if self.pod_index == 0:
                        # ì†ŒìŠ¤ ëª¨ë¸ì„ ì¶œë ¥ ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬
                        FileManager.copy_dir_contents(
                            self.path_manager.lora_source_model_path, 
                            self.path_manager.output_path / "source_model"
                        )
                    remove_unused_columns = False  # ë³‘í•©ëœ ëª¨ë¸ì€ ì»¬ëŸ¼ ì œê±° ë¶ˆí•„ìš”
                else:
                    if self.pod_index == 0:
                        # ì›ë³¸ ëª¨ë¸ì„ ì¶œë ¥ ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬
                        FileManager.copy_dir_contents(
                            self.path_manager.model_path, 
                            self.path_manager.output_path / "source_model"
                        )

                # ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ LoRA íƒ€ê²Ÿ ëª¨ë“ˆ ì„¤ì •
                target_modules = ModelManager.get_target_modules(model)
                lora_config = LoraConfig(
                    r=8,  # LoRA ë­í¬ (ì ì‘ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜)
                    lora_alpha=16,  # LoRA ìŠ¤ì¼€ì¼ë§ íŒ©í„°
                    lora_dropout=0.05,  # LoRA ë“œë¡­ì•„ì›ƒ
                    target_modules=target_modules  # LoRAë¥¼ ì ìš©í•  ëª¨ë“ˆë“¤
                )
                model = get_peft_model(model, lora_config)
                self.logger.info("LORA MODEL LOAD SUCCESS")

            self.logger.info("MODEL LOAD SUCCESS")
            return model, remove_unused_columns

        except Exception as e:
            traceback.print_exc()
            self.logger.error(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            update_finetuning_error_status()
            raise e

    def fine_tuning(self, args: FineTuningArgs):
        """íŒŒì¸íŠœë‹ ë©”ì¸ í”„ë¡œì„¸ìŠ¤ - ì „ì²´ íŒŒì¸íŠœë‹ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
        try:
            # ê²½ë¡œ ì„¤ì • ë° ë””ë ‰í† ë¦¬ ìƒì„±
            self.path_manager.ensure_directories()
            model_path = self.path_manager.get_model_path()

            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            tokenizer = self.load_tokenizer(model_path)

            # ëª¨ë¸ ë¡œë“œ (LoRA ì„¤ì • í¬í•¨)
            model, remove_unused_columns = self.load_model(model_path, args)

            # í•™ìŠµ ì¸ì ì„¤ì • (ì‚¬ìš©ì ì„¤ì • ë˜ëŠ” ê¸°ë³¸ê°’)
            training_args = self.training_manager.create_training_arguments(args, remove_unused_columns)
            self.logger.info("TRAINING ARGUMENT LOAD SUCCESS")

            # ë°ì´í„°ì…‹ ì¤€ë¹„ (í† í¬ë‚˜ì´ì§• ë° train/validation ë¶„í• )
            train_dataset, eval_dataset = DatasetManager.prepare_datasets(
                args.dataset_path, args.cutoff_length, tokenizer
            )
            self.logger.info("DATASET LOAD SUCCESS")

            # steps per epoch ê³„ì‚° (ë¶„ì‚° í•™ìŠµ ê³ ë ¤)
            steps_per_epoch = self.training_manager.calculate_steps_per_epoch(training_args, train_dataset)
            update_steps_per_epoch(steps_per_epoch=steps_per_epoch)

            self.logger.info("="*20)
            self.logger.info("TRAIN SETTING SUCCESS")
            self.logger.info("="*20)

            # ë°ì´í„° ì½œë ˆì´í„° ì„¤ì • (ì–¸ì–´ ëª¨ë¸ë§ìš©)
            data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

            # ëª¨ë¸ í•™ìŠµ ì‹¤í–‰
            trainer = self.training_manager.train_model(
                model, train_dataset, eval_dataset, training_args, tokenizer, data_collator
            )

            # ê²°ê³¼ ì €ì¥ (ì²« ë²ˆì§¸ podì—ì„œë§Œ ì‹¤í–‰)
            if self.pod_index == 0:
                trainer.save_model(training_args.output_dir)  # ìµœì¢… ëª¨ë¸ ì €ì¥
                FileManager.delete_checkpoint_dir(self.path_manager.output_path)  # ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬
                FileManager.clear_dir_contents(self.path_manager.model_path)  # ê¸°ì¡´ ëª¨ë¸ ì •ë¦¬
                FileManager.copy_dir_contents(self.path_manager.output_path, self.path_manager.model_path)  # ìƒˆ ëª¨ë¸ ë³µì‚¬

            self.logger.info("="*20)
            self.logger.info("FINE TUNING SUCCESS AND MODEL SAVE SUCCESS")
            self.logger.info("="*20)

        except Exception as e:
            traceback.print_exc()
            self.logger.error(f"íŒŒì¸íŠœë‹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            update_finetuning_error_status()
            raise e


def main():
    """ë©”ì¸ í•¨ìˆ˜ - ëª…ë ¹í–‰ ì¸ì íŒŒì‹± ë° íŒŒì¸íŠœë‹ ì—”ì§„ ì‹¤í–‰"""
    parser = argparse.ArgumentParser(description="Fine-tune a Hugging Face model.")
    parser.add_argument("--local_rank", type=int, default=0, required=True, 
                       help="í˜„ì¬ í”„ë¡œì„¸ìŠ¤ì˜ ë¡œì»¬ ë­í¬ (ë¶„ì‚° í•™ìŠµì—ì„œ ì‚¬ìš©)")
    parser.add_argument("--num_nodes", type=int, default=0, required=True, 
                       help="ë¶„ì‚° í•™ìŠµì— ì‚¬ìš©í•  ë…¸ë“œ ìˆ˜")
    parser.add_argument("--deepspeed", type=str, default=None, 
                       help="DeepSpeed ì„¤ì • íŒŒì¼ ê²½ë¡œ (ë¶„ì‚° í•™ìŠµ ìµœì í™”ìš©)")
    parser.add_argument("--num_gpus", type=int, default=0, required=True, 
                       help="ê° ë…¸ë“œì—ì„œ ì‚¬ìš©í•  GPU ìˆ˜")
    parser.add_argument("--dataset_path", type=str, required=True, 
                       help="í•™ìŠµì— ì‚¬ìš©í•  ë°ì´í„°ì…‹ ê²½ë¡œ (HuggingFace datasets í˜•ì‹)")
    parser.add_argument("--config_path", type=str, required=False, default=None, 
                       help="ì‚¬ìš©ì ì •ì˜ í•™ìŠµ ì„¤ì • íŒŒì¼ ê²½ë¡œ (JSON í˜•ì‹)")
    parser.add_argument("--num_train_epochs", type=float, default=3.0, 
                       help="ì „ì²´ í•™ìŠµ ì—í¬í¬ ìˆ˜")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1, 
                       help="ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í… ìˆ˜ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ ì‚¬ìš©)")
    parser.add_argument("--cutoff_length", type=int, default=128, 
                       help="ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ìµœëŒ€ í† í° ê¸¸ì´ (íŒ¨ë”©/íŠ¸ë ì¼€ì´ì…˜ ê¸°ì¤€)")
    parser.add_argument("--learning_rate", type=float, default=5e-5, 
                       help="í•™ìŠµë¥  (ê¸°ë³¸ê°’: 5e-5)")
    parser.add_argument("--warmup_steps", type=int, default=0, 
                       help="í•™ìŠµë¥  ì›Œë°ì—… ìŠ¤í… ìˆ˜ (ì ì§„ì  í•™ìŠµë¥  ì¦ê°€)")
    parser.add_argument("--used_lora", type=int, default=0, 
                       help="LoRA (Low-Rank Adaptation) ì‚¬ìš© ì—¬ë¶€ (0: ë¹„í™œì„±í™”, 1: í™œì„±í™”)")
    parser.add_argument("--used_dist", type=int, default=1, 
                       help="ë¶„ì‚° í•™ìŠµ ì‚¬ìš© ì—¬ë¶€ (0: ë¹„í™œì„±í™”, 1: í™œì„±í™”)")
    parser.add_argument("--load_in_8bit", type=int, default=0, 
                       help="8ë¹„íŠ¸ ì–‘ìí™”ë¡œ ëª¨ë¸ ë¡œë“œ ì—¬ë¶€ (ë©”ëª¨ë¦¬ ì ˆì•½ìš©, 0: ë¹„í™œì„±í™”, 1: í™œì„±í™”)")

    args = parser.parse_args()
    pod_index = int(os.getenv("POD_INDEX", 0))
    
    # ì²« ë²ˆì§¸ podì—ì„œë§Œ ì´ˆê¸°í™” ì‘ì—… ìˆ˜í–‰
    if pod_index == 0:
        update_finetuning_running_status()  # íŒŒì¸íŠœë‹ ìƒíƒœ ì—…ë°ì´íŠ¸
        path_manager = PathManager()
        FileManager.clear_dir_contents(path_manager.output_path)  # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì •ë¦¬
        FileManager.clear_dir_contents(path_manager.log_path)  # ë¡œê·¸ ë””ë ‰í† ë¦¬ ì •ë¦¬

    # FineTuningArgs ê°ì²´ ìƒì„±
    fine_tuning_args = FineTuningArgs(
        local_rank=args.local_rank,
        num_nodes=args.num_nodes,
        num_gpus=args.num_gpus,
        dataset_path=args.dataset_path,
        config_path=args.config_path,
        num_train_epochs=args.num_train_epochs,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        cutoff_length=args.cutoff_length,
        learning_rate=args.learning_rate,
        warmup_steps=args.warmup_steps,
        used_lora=args.used_lora,
        used_dist=args.used_dist,
        load_in_8bit=args.load_in_8bit,
        deepspeed=args.deepspeed
    )

    # íŒŒì¸íŠœë‹ ì—”ì§„ ì‹¤í–‰
    engine = FineTuningEngine()
    engine.logger.info(fine_tuning_args)
    
    try:
        engine.fine_tuning(fine_tuning_args)
    except Exception:
        if pod_index == 0:
            update_finetuning_error_status()  # ì˜¤ë¥˜ ìƒíƒœ ì—…ë°ì´íŠ¸


if __name__ == "__main__":
    main()


"""
================================================================================
ğŸ“‹ ì „ì²´ íŒŒì¸íŠœë‹ ì›Œí¬í”Œë¡œìš° íë¦„ ì„¤ëª…
================================================================================

ğŸ¯ ëª©ì : Hugging Face ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸ì˜ íš¨ìœ¨ì ì¸ íŒŒì¸íŠœë‹ì„ ìœ„í•œ í†µí•© ì—”ì§„

ğŸ”„ ì „ì²´ í”„ë¡œì„¸ìŠ¤ íë¦„:

1ï¸âƒ£ ì´ˆê¸°í™” ë‹¨ê³„ (main í•¨ìˆ˜)
   â”œâ”€â”€ ëª…ë ¹í–‰ ì¸ì íŒŒì‹± (ë¶„ì‚° í•™ìŠµ ì„¤ì •, ë°ì´í„°ì…‹ ê²½ë¡œ, LoRA ì˜µì…˜ ë“±)
   â”œâ”€â”€ pod_index í™•ì¸ (ë¶„ì‚° í•™ìŠµì—ì„œ í˜„ì¬ pod ì‹ë³„)
   â””â”€â”€ ì²« ë²ˆì§¸ podì—ì„œë§Œ ì´ˆê¸°í™” ì‘ì—… ìˆ˜í–‰
       â”œâ”€â”€ íŒŒì¸íŠœë‹ ìƒíƒœ ì—…ë°ì´íŠ¸ (DB)
       â”œâ”€â”€ ì¶œë ¥ ë””ë ‰í† ë¦¬ ì •ë¦¬ (/tmp_model)
       â””â”€â”€ ë¡œê·¸ ë””ë ‰í† ë¦¬ ì •ë¦¬ (/logs)

2ï¸âƒ£ ê²½ë¡œ ë° í™˜ê²½ ì„¤ì • (PathManager)
   â”œâ”€â”€ ëª¨ë¸ ê²½ë¡œ ê²°ì • (/model ë˜ëŠ” /model/source_model)
   â”œâ”€â”€ LoRA ì†ŒìŠ¤ ëª¨ë¸ ìš°ì„  ì‚¬ìš© (ì¡´ì¬ ì‹œ)
   â””â”€â”€ í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±

3ï¸âƒ£ í† í¬ë‚˜ì´ì € ë¡œë“œ (load_tokenizer)
   â”œâ”€â”€ AutoTokenizerë¡œ ëª¨ë¸ë³„ í† í¬ë‚˜ì´ì € ìë™ ë¡œë“œ
   â”œâ”€â”€ íŒ¨ë”© í† í°ì„ EOS í† í°ìœ¼ë¡œ ì„¤ì • (ì–¸ì–´ ëª¨ë¸ë§ìš©)
   â””â”€â”€ ì˜¤ë¥˜ ë°œìƒ ì‹œ DB ìƒíƒœ ì—…ë°ì´íŠ¸

4ï¸âƒ£ ëª¨ë¸ ë¡œë“œ ë° LoRA ì„¤ì • (load_model)
   â”œâ”€â”€ ëª¨ë¸ ë¡œë“œ ì˜µì…˜
   â”‚   â”œâ”€â”€ 8ë¹„íŠ¸ ì–‘ìí™” (ë©”ëª¨ë¦¬ ì ˆì•½)
   â”‚   â””â”€â”€ ì¼ë°˜ ë¡œë“œ
   â”œâ”€â”€ LoRA ì²˜ë¦¬ ë¡œì§
   â”‚   â”œâ”€â”€ ê¸°ì¡´ LoRA ëª¨ë¸ì¸ ê²½ìš°
   â”‚   â”‚   â”œâ”€â”€ PeftModel ë¡œë“œ
   â”‚   â”‚   â”œâ”€â”€ merge_and_unload()ë¡œ ê°€ì¤‘ì¹˜ ë³‘í•©
   â”‚   â”‚   â””â”€â”€ ì†ŒìŠ¤ ëª¨ë¸ ë³µì‚¬
   â”‚   â””â”€â”€ ìƒˆ LoRA ì ìš©ì¸ ê²½ìš°
   â”‚       â”œâ”€â”€ ëª¨ë¸ íƒ€ì…ë³„ íƒ€ê²Ÿ ëª¨ë“ˆ ìë™ íƒì§€
   â”‚       â”œâ”€â”€ LoraConfig ì„¤ì • (r=8, alpha=16, dropout=0.05)
   â”‚       â””â”€â”€ get_peft_model()ë¡œ LoRA ì ìš©
   â””â”€â”€ remove_unused_columns í”Œë˜ê·¸ ê²°ì •

5ï¸âƒ£ í•™ìŠµ ì¸ì ì„¤ì • (create_training_arguments)
   â”œâ”€â”€ ì‚¬ìš©ì ì„¤ì • íŒŒì¼ ì‚¬ìš© (config_path ì œê³µ ì‹œ)
   â”‚   â”œâ”€â”€ JSON ì„¤ì • íŒŒì¼ ë¡œë“œ
   â”‚   â”œâ”€â”€ cutoff_length ì œê±° (í† í¬ë‚˜ì´ì§•ì—ì„œ ì²˜ë¦¬ë¨)
   â”‚   â””â”€â”€ ì¤‘ë³µ íŒŒë¼ë¯¸í„° í•„í„°ë§
   â””â”€â”€ ê¸°ë³¸ DeepSpeed ì„¤ì • ì‚¬ìš©
       â”œâ”€â”€ learning_rate, epochs, warmup_steps ë“±
       â””â”€â”€ DeepSpeed ì„¤ì • íŒŒì¼ ê²½ë¡œ ì§€ì •

6ï¸âƒ£ ë°ì´í„°ì…‹ ì¤€ë¹„ (prepare_datasets)
   â”œâ”€â”€ ë°ì´í„°ì…‹ ë¡œë“œ
   â”‚   â”œâ”€â”€ ë””ë ‰í† ë¦¬ ê¸°ë°˜ Arrow í˜•ì‹
   â”‚   â”œâ”€â”€ JSON/JSONL íŒŒì¼
   â”‚   â””â”€â”€ CSV íŒŒì¼
   â”œâ”€â”€ train/validation ë¶„í• 
   â”‚   â”œâ”€â”€ validationì´ ìˆìœ¼ë©´ validation ì‚¬ìš©, ì—†ìœ¼ë©´ test ì‚¬ìš©
   â”‚   â””â”€â”€ validationì´ ì—†ìœ¼ë©´ ì˜¤ë¥˜ ë°œìƒ
   â”œâ”€â”€ í† í¬ë‚˜ì´ì§•
   â”‚   â”œâ”€â”€ íŒ¨ë”©, íŠ¸ë ì¼€ì´ì…˜, attention_mask ìƒì„±
   â”‚   â””â”€â”€ labelsë¥¼ input_idsì™€ ë™ì¼í•˜ê²Œ ì„¤ì • (ì–¸ì–´ ëª¨ë¸ë§)
   â””â”€â”€ PyTorch í…ì„œ í˜•íƒœë¡œ í¬ë§· ì„¤ì •

7ï¸âƒ£ í•™ìŠµ ìŠ¤ì¼€ì¤„ë§ (calculate_steps_per_epoch)
   â”œâ”€â”€ ë¶„ì‚° í•™ìŠµ ê³ ë ¤
   â”‚   â”œâ”€â”€ world_size (ì´ ë””ë°”ì´ìŠ¤ ìˆ˜)
   â”‚   â”œâ”€â”€ per_device_batch_size
   â”‚   â””â”€â”€ gradient_accumulation_steps
   â”œâ”€â”€ ì‹¤ì œ ë°°ì¹˜ í¬ê¸° ê³„ì‚°
   â”‚   â””â”€â”€ effective_batch_size = batch_size Ã— world_size Ã— accumulation
   â””â”€â”€ epochë‹¹ ìŠ¤í… ìˆ˜ ê³„ì‚°
       â””â”€â”€ steps_per_epoch = total_samples Ã· effective_batch_size

8ï¸âƒ£ ëª¨ë¸ í•™ìŠµ ì‹¤í–‰ (train_model)
   â”œâ”€â”€ Trainer ì„¤ì •
   â”‚   â”œâ”€â”€ DeepSpeed í˜¸í™˜ì„± ì„¤ì • (pin_memory=False, num_workers=0)
   â”‚   â”œâ”€â”€ optimizers=None (DeepSpeedê°€ ê´€ë¦¬)
   â”‚   â””â”€â”€ SavePeftAdapterCallback ì¶”ê°€ (LoRA ì–´ëŒ‘í„° ì €ì¥)
   â”œâ”€â”€ í•™ìŠµ ì‹œì‘
   â”‚   â”œâ”€â”€ trainer.can_return_loss = True
   â”‚   â””â”€â”€ trainer.train()
   â””â”€â”€ í•™ìŠµ ì™„ë£Œ

9ï¸âƒ£ ê²°ê³¼ ì €ì¥ ë° ì •ë¦¬ (ì²« ë²ˆì§¸ podì—ì„œë§Œ)
   â”œâ”€â”€ ìµœì¢… ëª¨ë¸ ì €ì¥ (trainer.save_model)
   â”œâ”€â”€ ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬ (checkpoint-* ë””ë ‰í† ë¦¬ ì‚­ì œ)
   â”œâ”€â”€ ê¸°ì¡´ ëª¨ë¸ ì •ë¦¬ (/model ë””ë ‰í† ë¦¬ ë¹„ìš°ê¸°)
   â””â”€â”€ ìƒˆ ëª¨ë¸ ë³µì‚¬ (/tmp_model â†’ /model)

ğŸ”§ í•µì‹¬ ê¸°ìˆ ì  íŠ¹ì§•:

â€¢ ë¶„ì‚° í•™ìŠµ ì§€ì›: DeepSpeedë¥¼ í†µí•œ ë‹¤ì¤‘ GPU/ë…¸ë“œ í•™ìŠµ
â€¢ LoRA íš¨ìœ¨ì„±: ì ì€ íŒŒë¼ë¯¸í„°ë¡œ ëŒ€í˜• ëª¨ë¸ íŒŒì¸íŠœë‹
â€¢ ë©”ëª¨ë¦¬ ìµœì í™”: 8ë¹„íŠ¸ ì–‘ìí™”, 16ë¹„íŠ¸ ì •ë°€ë„, ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì 
â€¢ ìë™í™”: ëª¨ë¸ íƒ€ì…ë³„ LoRA íƒ€ê²Ÿ ëª¨ë“ˆ ìë™ íƒì§€
â€¢ ì˜¤ë¥˜ ì²˜ë¦¬: ê° ë‹¨ê³„ë³„ ì˜ˆì™¸ ì²˜ë¦¬ ë° DB ìƒíƒœ ì—…ë°ì´íŠ¸
â€¢ ì¬í˜„ ê°€ëŠ¥ì„±: ê³ ì •ëœ seedì™€ ì²´ê³„ì ì¸ ë¡œê¹…

ğŸ“Š ì„±ëŠ¥ ìµœì í™” í¬ì¸íŠ¸:

â€¢ ë°°ì¹˜ í¬ê¸°: VRAM ìš©ëŸ‰ì— ë”°ë¥¸ ìë™ ìµœì í™”
â€¢ í•™ìŠµë¥ : ì›Œë°ì—…ê³¼ ìŠ¤ì¼€ì¤„ë§ ì§€ì›
â€¢ ì²´í¬í¬ì¸íŠ¸: ì €ì¥ ì œí•œìœ¼ë¡œ ë””ìŠ¤í¬ ê³µê°„ ì ˆì•½
â€¢ ë°ì´í„° ë¡œë”©: ë°°ì¹˜ ì²˜ë¦¬ì™€ ë©”ëª¨ë¦¬ íš¨ìœ¨ì  í† í¬ë‚˜ì´ì§•

ğŸš€ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:

1. ë‹¨ì¼ GPU íŒŒì¸íŠœë‹: used_dist=0, used_lora=1
2. ë‹¤ì¤‘ GPU íŒŒì¸íŠœë‹: used_dist=1, num_gpus=N
3. ê¸°ì¡´ LoRA ëª¨ë¸ ì—°ì† í•™ìŠµ: lora_source_model_path ì‚¬ìš©
4. ë©”ëª¨ë¦¬ ì œì•½ í™˜ê²½: load_in_8bit=1, gradient_accumulation_steps=N

================================================================================
"""