import asyncio
import logging
from typing import List, Dict
from utils import topic_key, settings, redis_key, mongodb_key
from utils.redis import get_redis_client_async
from utils.mongodb import mongodb_conn
from datetime import datetime
import time
import random
from fastapi import FastAPI
from fastapi.responses import JSONResponse
import uvicorn
import threading

from aiokafka import AIOKafkaConsumer, AIOKafkaProducer
from confluent_kafka.admin import AdminClient, NewPartitions, NewTopic
import json

logger = logging.getLogger("alert-server")
logging.basicConfig(level=logging.INFO)

app = FastAPI()

class ConnectionManager:
    def __init__(self):
        self.max_retries = 5
        self.base_delay = 1  # ì´ˆê¸° ì§€ì—° ì‹œê°„ (ì´ˆ)
        self.max_delay = 30  # ìµœëŒ€ ì§€ì—° ì‹œê°„ (ì´ˆ)
        self.kafka_healthy = False

    async def connect_with_retry(self, connect_func, *args, **kwargs):
        retry_count = 0
        while True:
            try:
                result = await connect_func(*args, **kwargs)
                self.kafka_healthy = True
                return result
            except Exception as e:
                retry_count += 1
                if retry_count > self.max_retries:
                    logger.error(f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {str(e)}")
                    self.kafka_healthy = False
                    raise

                # ì§€ìˆ˜ ë°±ì˜¤í”„ ê³„ì‚° (jitter ì¶”ê°€)
                delay = min(self.base_delay * (2 ** (retry_count - 1)), self.max_delay)
                jitter = random.uniform(0, 0.1 * delay)
                total_delay = delay + jitter

                logger.warning(f"ì—°ê²° ì‹¤íŒ¨, {total_delay:.2f}ì´ˆ í›„ ì¬ì‹œë„ ({retry_count}/{self.max_retries}): {str(e)}")
                await asyncio.sleep(total_delay)

    async def get_redis_client(self):
        return await self.connect_with_retry(get_redis_client_async)

    async def create_kafka_consumer(self, topics, group_id):
        async def _create_consumer():
            consumer = AIOKafkaConsumer(
                *topics,
                bootstrap_servers=settings.JF_KAFKA_DNS,
                value_deserializer=lambda m: m.decode('utf-8'),
                group_id=group_id
            )
            await consumer.start()
            return consumer
        return await self.connect_with_retry(_create_consumer)

connection_manager = ConnectionManager()

@app.get("/health")
async def health_check():
    """Health check endpoint for probes"""
    return JSONResponse(
        content={"status": "healthy", "kafka_healthy": connection_manager.kafka_healthy},
        status_code=200
    )

@app.get("/ready")
async def readiness_check():
    """Readiness check endpoint"""
    return JSONResponse(
        content={"status": "ready", "kafka_healthy": connection_manager.kafka_healthy},
        status_code=200
    )

# í† í”½ì´ ìˆëŠ”ì§€ ê²€ì‚¬
async def check_topic():
    try:
        conf = {
            'bootstrap.servers': settings.JF_KAFKA_DNS
        }
        a = AdminClient(conf)
        topic_metadata = a.list_topics(timeout=10)
        not_found_topics = []
        for topic in topic_key.ALERT_TOPICS:
            if topic not in topic_metadata.topics:
                logger.error(f"âŒ Topic '{topic}' not found.")
                not_found_topics.append(topic)
            else:
                logger.info(f"âœ… Topic '{topic}' found.")
        connection_manager.kafka_healthy = True
        return not_found_topics
    except Exception as e:
        logger.error(f"Kafka ì—°ê²° ì‹¤íŒ¨: {str(e)}")
        connection_manager.kafka_healthy = False
        return []

# ì—†ëŠ” í† í”½ ìƒì„±
async def create_topic(topics: List[str] = topic_key.ALERT_TOPICS):
    if not topics:
        logger.info("âœ… All topics are already created.")
        return
    try:
        conf = {
            'bootstrap.servers': settings.JF_KAFKA_DNS
        }
        a = AdminClient(conf)
        new_topics = [topic for topic in topics if topic not in a.list_topics().topics]
        if new_topics:
            fs = a.create_topics([NewTopic(topic=topic, num_partitions=3, replication_factor=1) for topic in new_topics])
            for topic, f in fs.items():
                try:
                    f.result()
                    logger.info(f"âœ… Topic '{topic}' created successfully.")
                except Exception as e:
                    logger.error(f"âŒ Failed to create topic '{topic}': {e}")
        else:
            logger.info("âœ… All topics are already created.")
        connection_manager.kafka_healthy = True
    except Exception as e:
        logger.error(f"í† í”½ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        connection_manager.kafka_healthy = False

# í† í”½ì˜ íŒŒí‹°ì…˜ ê°œìˆ˜ ëŠ˜ë¦¬ê¸°
def increase_partition():
    conf = {
        'bootstrap.servers': settings.JF_KAFKA_DNS
    }
    a = AdminClient(conf)
    new_partition_count = 3
    new_partitions = [NewPartitions(topic=topic, new_total_count=new_partition_count) for topic in topic_key.ALERT_TOPICS]
    fs = a.create_partitions(new_partitions)
    for topic, f in fs.items():
        try:
            f.result()
            logger.info(f"âœ… Partitions for topic '{topic}' increased successfully.")
        except Exception as e:
            logger.error(f"âŒ Failed to increase partitions for topic '{topic}': {e}")


async def send_alert(user_id: str, alert: dict, redis_client = None):
    try:
        if not redis_client:
            redis_client = await connection_manager.get_redis_client()
        key = redis_key.USER_ALERT_CHANNEL.format(user_id)
        await redis_client.xadd(key, alert)
        logger.info(f"ğŸ”” Alert sent to {key}")
        logger.info(f"ğŸ”” Alert data: {alert}")
    except Exception as e:
        logger.error(f"Alert ì „ì†¡ ì‹¤íŒ¨: {str(e)}")
        raise

async def save_alert_to_mongodb(alert: dict):
    with mongodb_conn(database_name=mongodb_key.DATABASE, collection_name=mongodb_key.NOTIFICATION_COLLECTION) as collection:
        alert["create_datetime"] = datetime.now()
        collection.insert_one(alert)
    logging.info(f"âœ… Alert saved to MongoDB")
    return True

async def alert_consume():
    while True:
        try:
            consumer = await connection_manager.create_kafka_consumer(
                topic_key.ALERT_TOPICS,
                "mlops-alert-group"
            )
            logger.info("ğŸ”” Alert consumer started")
            
            while True:
                try:
                    async for msg in consumer:
                        try:
                            data = json.loads(msg.value)
                            user_id = data.get("user_id")
                            if user_id:
                                await send_alert(user_id, data)
                                await save_alert_to_mongodb(data)
                        except Exception as e:
                            logger.error(f"ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                except Exception as e:
                    logger.error(f"Consumer ë£¨í”„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    break
        except Exception as e:
            logger.error(f"Consumer ì‹œì‘ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            await asyncio.sleep(5)  # ì‹¬ê°í•œ ì˜¤ë¥˜ ì‹œ ì ì‹œ ëŒ€ê¸°
        finally:
            try:
                await consumer.stop()
            except:
                pass

async def user_alert_log_delete(user_id: int):
    try:
        with mongodb_conn(database_name=mongodb_key.DATABASE, collection_name=mongodb_key.NOTIFICATION_COLLECTION) as collection:
            collection.delete_many({"user_id": user_id})
            logging.info(f"âœ… Alert logs deleted for user {user_id}")
            return True
    except Exception as e:
        logging.error(f"âŒ Error deleting alert logs for user {user_id}: {e}")
        return False
    except:
        logging.error(f"âŒ Error deleting alert logs for user {user_id}: Unknown error")
        return False


async def user_delete_consume():
    while True:
        try:
            consumer = await connection_manager.create_kafka_consumer(
                [topic_key.USER_DELETE_TOPIC],
                "my-group"
            )
            logger.info("ğŸ”” User delete consumer started")
            
            while True:
                try:
                    async for msg in consumer:
                        try:
                            data = json.loads(msg.value)
                            user_id = data.get("user_id")
                            if user_id:
                                await user_alert_log_delete(user_id)
                                redis_client = await connection_manager.get_redis_client()
                                key = redis_key.USER_ALERT_CHANNEL.format(user_id)
                                await redis_client.delete(key)
                                logger.info(f"âœ… Redis stream deleted for user {user_id}")
                        except Exception as e:
                            logger.error(f"ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                except Exception as e:
                    logger.error(f"Consumer ë£¨í”„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    break
        except Exception as e:
            logger.error(f"Consumer ì‹œì‘ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            await asyncio.sleep(5)  # ì‹¬ê°í•œ ì˜¤ë¥˜ ì‹œ ì ì‹œ ëŒ€ê¸°
        finally:
            try:
                await consumer.stop()
            except:
                pass

async def main():
    # ì´ˆê¸°í™” ì‹œ Kafka ì—°ê²° ì²´í¬ (ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰)
    logger.info("ğŸš€ Starting alert management service...")
    not_found_topics = await check_topic()
    if not_found_topics:
        await create_topic(not_found_topics)
    
    await asyncio.gather(
        alert_consume(),
        user_delete_consume()
    )

def run_fastapi():
    """FastAPI ì„œë²„ë¥¼ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰"""
    import logging
    
    # Health check ê²½ë¡œì— ëŒ€í•œ ë¡œê·¸ë¥¼ í•„í„°ë§í•˜ëŠ” ì»¤ìŠ¤í…€ í•„í„°
    class HealthCheckFilter:
        def filter(self, record):
            return not ('/health' in record.getMessage() or '/ready' in record.getMessage())
    
    # uvicorn access loggerì— í•„í„° ì¶”ê°€
    uvicorn_access = logging.getLogger("uvicorn.access")
    uvicorn_access.addFilter(HealthCheckFilter())
    
    uvicorn.run(app, host="0.0.0.0", port=8000)

if __name__ == "__main__":
    # FastAPI ì„œë²„ë¥¼ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘
    fastapi_thread = threading.Thread(target=run_fastapi, daemon=True)
    fastapi_thread.start()
    
    # ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰
    asyncio.run(main())