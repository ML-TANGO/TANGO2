import sys
from langchain_ollama import OllamaLLM
from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate, ChatPromptTemplate, MessagesPlaceholder
from langchain_classic.chains import LLMChain
from langchain_classic.memory import ConversationBufferMemory

# ëª¨ë“ˆí™”ëœ í•¨ìˆ˜ë“¤ ì„í¬íŠ¸
from src.config import read_config
from src.ollama_manager import start_ollama, stop_ollama, pull_model_if_needed
from src.data_loader import load_examples
from src.prompt_loader import load_prompt_config, load_prompt_components
from src.app_ui import run_cli, run_web

def main():
    """Main function to run the application."""
    ollama_status = None
    ollama_process = None
    model_name = None
    try:
        # 1. ì„¤ì • ì½ê¸°
        config = read_config()
        ui_mode = config.get('app', 'UI_MODE', fallback='cli')
        interaction_mode = config.get('app', 'INTERACTION_MODE', fallback='single_shot')
        model_name = config.get('app', 'MODEL_NAME', fallback='gemma3')
        web_host = config.get('web', 'HOST', fallback='127.0.0.1')
        web_port = config.getint('web', 'PORT', fallback=5000)

        # DSPy Configure
        dspy_optimizer = config.get('DSPY', 'OPTIMIZER', fallback='BootstrapFewShot')
        use_dspy = config.getboolean('DSPY', 'USE_DSPY', fallback=False)
        dspy_metric = config.get('DSPY', 'METRIC', fallback='bert_score')

        print(f"--- Mode: {ui_mode.upper()} | Interaction: {interaction_mode.upper()} ---")
        if use_dspy and interaction_mode == 'single_shot':
            print(f"--- DSPy Optimizer: {dspy_optimizer} ---")

        # 2. Ollama ì„œë²„ ë° ëª¨ë¸ ì¤€ë¹„
        ollama_status, ollama_process = start_ollama()
        if ollama_status is None:
            return

        if not pull_model_if_needed(model_name):
            print(f"'{model_name}' ëª¨ë¸ì„ ì¤€ë¹„í•  ìˆ˜ ì—†ì–´ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return

        # 3. Few-Shot ì˜ˆì‹œ ë¡œë“œ
        examples = []
        load_examples_needed = True #TODO: í•­ìƒ í•„ìš”í•œì§€ ê³ ë¯¼
        if load_examples_needed:
            print("="*50)
            print("ğŸ¤– Few-Shot ì˜ˆì‹œë¡œ ì‚¬ìš©í•  CSV íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            print("   - ê²½ë¡œ ì…ë ¥ì„ ë§ˆì¹˜ë ¤ë©´ ê·¸ëƒ¥ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”.")
            print("="*50)
            input_csv_paths, output_csv_paths = [], []
            while True:
                path = input(f"ì…ë ¥(Input) CSV íŒŒì¼ ê²½ë¡œ #{len(input_csv_paths) + 1}: ")
                if not path: break
                input_csv_paths.append(path)
            if input_csv_paths:
                while len(output_csv_paths) < len(input_csv_paths):
                    path = input(f"ì¶œë ¥(Output) CSV íŒŒì¼ ê²½ë¡œ #{len(output_csv_paths) + 1}: ")
                    if not path: break
                    output_csv_paths.append(path)

            if input_csv_paths and output_csv_paths:
                shuffle_choice = input("ğŸ”€ ì˜ˆì œë¥¼ ì…”í”Œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n, ê¸°ë³¸ê°’ n): ").lower().strip()
                do_shuffle = shuffle_choice == 'y'

                examples = load_examples(input_csv_paths, output_csv_paths, shuffle=do_shuffle)

                if examples:
                    print(f"âœ… ì´ {len(examples)}ê°œì˜ Few-Shot ì˜ˆì‹œë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
                else:
                    print("âš ï¸  ë¶ˆëŸ¬ì˜¨ ì˜ˆì‹œê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                print("ğŸ’¡ ì…ë ¥ëœ Few-Shot ì˜ˆì‹œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

        # 4. LLM ë° ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
        llm = OllamaLLM(model=model_name)
        context = {}

        if interaction_mode == 'conversational': # conversational ëª¨ë“œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
            conv_templates = load_prompt_config("prompts/conversational.yml")
            system_message = conv_templates['system_message_prefix'] + "\n\n"

            example_template_str = conv_templates['example_template']
            for ex in examples:
                system_message += f"Query: {ex['query']}\nResponse: {ex['response']}\n\n"

            memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
            prompt = ChatPromptTemplate.from_messages([
                ("system", system_message),
                MessagesPlaceholder(variable_name="chat_history"),
                ("human", "{user_query}"),
            ])
            chain = LLMChain(llm=llm, prompt=prompt, memory=memory, verbose=True)
            context = {"mode": "conversational", "chain": chain}

        else: # single_shot ëª¨ë“œ
            if use_dspy and examples: # DSPy ì‚¬ìš© ë¡œì§
                print("\nğŸš€ DSPy ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ë¥¼ ì»´íŒŒì¼í•©ë‹ˆë‹¤...")
                print("   (MIPROv2ì™€ ê°™ì€ ì¼ë¶€ ì˜µí‹°ë§ˆì´ì €ëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)")
                try:
                    from src.dspy_handler import compile_program, print_program_details
                    compiled_program = compile_program(model_name, dspy_optimizer, examples, metric_name=dspy_metric)
                    print("âœ… DSPy í”„ë¡œê·¸ë¨ ì»´íŒŒì¼ ì™„ë£Œ!")

                    # ì»´íŒŒì¼ëœ í”„ë¡œê·¸ë¨ì˜ ìƒì„¸ ë‚´ìš©(ì§€ì‹œë¬¸, ì˜ˆì œ) ì¶œë ¥
                    print_program_details(compiled_program)

                    context = {
                        "mode": "dspy_single_shot",
                        "dspy_program": compiled_program,
                    }
                except ImportError as e:
                    print(f"\n[ì—ëŸ¬] DSPy ê´€ë ¨ ëª¨ë“ˆì„ ì„í¬íŠ¸í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
                    print("   'pip install dspy-ai'ë¥¼ ì‹¤í–‰í•˜ì—¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
                    return
                except Exception as e:
                    print(f"\n[ì—ëŸ¬] DSPy í”„ë¡œê·¸ë¨ ì»´íŒŒì¼ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                    return
            else: # ê¸°ì¡´ LangChain single_shot ë¡œì§
                if use_dspy and not examples:
                    print("\nDSPyë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •ë˜ì—ˆì§€ë§Œ, Few-shot ì˜ˆì œê°€ ì œê³µë˜ì§€ ì•Šì•„ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")

                prompt_components = load_prompt_components()
                context = {
                    "mode": "single_shot",
                    "llm": llm,
                    "prompt_components": prompt_components,
                    "examples": examples
                }

        # 5. UI ì‹¤í–‰
        if ui_mode == 'web':
            run_web(context, web_host, web_port)
        else:
            run_cli(context)

    except FileNotFoundError as e:
        print(f"\nì˜¤ë¥˜: {e}. ì„¤ì • íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    except KeyboardInterrupt:
        print("\nì‚¬ìš©ìì— ì˜í•´ í”„ë¡œê·¸ë¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    finally:
        # Unload the model and/or stop the server
        if ollama_status:
            stop_ollama(ollama_status, ollama_process, model_name)
        print("Application shut down.")

if __name__ == "__main__":
    main()
