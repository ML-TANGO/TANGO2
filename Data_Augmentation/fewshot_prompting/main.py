import sys
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate, FewShotPromptTemplate, ChatPromptTemplate, MessagesPlaceholder
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferMemory

from src.config import read_config
from src.ollama_manager import start_ollama, stop_ollama, pull_model_if_needed
from src.data_loader import load_examples
from src.app_ui import run_cli, run_web

def main():
    """Main function to run the application."""
    ollama_status = None
    model_name = None
    try:
        # 1. ì„¤ì • ì½ê¸°
        config = read_config()
        ui_mode = config.get('app', 'UI_MODE', fallback='cli')
        interaction_mode = config.get('app', 'INTERACTION_MODE', fallback='single_shot')
        model_name = config.get('app', 'MODEL_NAME', fallback='gemma3')
        web_host = config.get('web', 'HOST', fallback='127.0.0.1')
        web_port = config.getint('web', 'PORT', fallback=5000)

        print(f"--- Mode: {ui_mode.upper()} | Interaction: {interaction_mode.upper()} ---")

        # 2. Ollama ì„œë²„ ë° ëª¨ë¸ ì¤€ë¹„
        ollama_status = start_ollama()
        if ollama_status is None:
            # start_ollama now prints a detailed error message
            return

        if not pull_model_if_needed(model_name):
            print(f"'{model_name}' ëª¨ë¸ì„ ì¤€ë¹„í•  ìˆ˜ ì—†ì–´ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return

        # 3. Few-Shot ì˜ˆì‹œ ë¡œë“œ
        examples = []
        print("="*50)
        print("ğŸ¤– Few-Shot ì˜ˆì‹œë¡œ ì‚¬ìš©í•  CSV íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        print("   - ê²½ë¡œ ì…ë ¥ì„ ë§ˆì¹˜ë ¤ë©´ ê·¸ëƒ¥ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”.")
        print("="*50)
        input_csv_paths, output_csv_paths = [], []
        while True:
            path = input(f"ì…ë ¥(Input) CSV íŒŒì¼ ê²½ë¡œ #{len(input_csv_paths) + 1}: ")
            if not path: break
            input_csv_paths.append(path)
        if input_csv_paths:
            while len(output_csv_paths) < len(input_csv_paths):
                path = input(f"ì¶œë ¥(Output) CSV íŒŒì¼ ê²½ë¡œ #{len(output_csv_paths) + 1}: ")
                if not path: break
                output_csv_paths.append(path)
        
        if input_csv_paths and output_csv_paths:
            examples = load_examples(input_csv_paths, output_csv_paths)
            if examples: print(f"âœ… ì´ {len(examples)}ê°œì˜ Few-Shot ì˜ˆì‹œë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
            else: print("âš ï¸  ë¶ˆëŸ¬ì˜¨ ì˜ˆì‹œê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            print("ğŸ’¡ ì…ë ¥ëœ Few-Shot ì˜ˆì‹œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

        # 4. LLM ë° ì²´ì¸ ì„¤ì •
        llm = Ollama(model=model_name)
        chain = None

        if interaction_mode == 'conversational':
            system_message = "You are a helpful assistant. Based on the following examples, have a conversation.\n\n"
            for ex in examples:
                system_message += f"Query: {ex['query']}\nResponse: {ex['response']}\n\n"
            
            memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
            prompt = ChatPromptTemplate.from_messages([
                ("system", system_message),
                MessagesPlaceholder(variable_name="chat_history"),
                ("human", "{user_query}"),
            ])
            chain = LLMChain(llm=llm, prompt=prompt, memory=memory, verbose=True)
        else: # single_shot ëª¨ë“œ
            example_prompt = PromptTemplate.from_template("Query: {query}\nResponse: {response}")
            prompt = FewShotPromptTemplate(
                examples=examples,
                example_prompt=example_prompt,
                prefix="Here are some examples. Please follow this format.",
                suffix="Now, answer the following query:\nQuery: {user_query}",
                input_variables=["user_query"],
                example_separator="\n\n"
            )
            chain = LLMChain(llm=llm, prompt=prompt, verbose=True)

        # 5. UI ì‹¤í–‰
        if ui_mode == 'web':
            run_web(chain, interaction_mode, web_host, web_port)
        else:
            run_cli(chain, interaction_mode)

    except FileNotFoundError as e:
        print(f"\nì˜¤ë¥˜: {e}. ì„¤ì • íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    except KeyboardInterrupt:
        print("\nì‚¬ìš©ìì— ì˜í•´ í”„ë¡œê·¸ë¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    finally:
        # Unload the model from memory via API
        if ollama_status:
            stop_ollama(model_name)
        print("Application shut down.")

if __name__ == "__main__":
    main()
