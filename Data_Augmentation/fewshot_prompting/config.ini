[app]
# UI_MODE: 사용할 사용자 인터페이스를 선택합니다.
# 사용 가능 옵션: cli, web
UI_MODE = cli

# INTERACTION_MODE: LLM과의 상호작용 방식을 선택합니다.
# single_shot: 하나의 질문에 하나의 전체 답변을 받습니다.
# conversational: 실시간 대화 방식. 답변이 스트리밍되고 이전 대화를 기억합니다.
INTERACTION_MODE = single_shot

# MODEL_NAME: 사용할 Ollama 모델의 이름을 지정합니다.
# 예시: gemma3, llama2, mistral 등 Ollama에서 지원하는 모델
MODEL_NAME = gemma3:27b

[web]
# HOST: 웹 서버를 실행할 호스트 주소입니다.
HOST = 0.0.0.0

# PORT: 웹 서버가 사용할 포트 번호입니다.
PORT = 5001

[DSPY]
# USE_DSPY: DSPy 프롬프트 옵티마이저 사용 여부를 결정합니다. (yes/no)
# 'yes'로 설정하면, Few-shot 예시가 있을 경우 DSPy를 사용해 프롬프트를 최적화합니다.
# 'no'로 설정하면, 주어진 프롬프트를 그대로 사용합니다.
USE_DSPY = yes

# OPTIMIZER: 사용할 프롬프트 옵티마이저를 선택합니다.
# BootstrapFewShot: 기본적인 예제(example) 선택기입니다.
# BootstrapFewShotWithRandomSearch: 대규모 예제 후보군에서 탐색합니다.
# MIPROv2: 예제와 지시문(instruction)을 동시에 최적화합니다. (권장)
OPTIMIZER = MIPROv2

# METRIC: 최적화 과정에서 사용할 평가 지표를 선택합니다.
# bert_score: BERT 모델을 사용하여 의미적 유사도를 평가합니다. (권장)
# jaccard: 단어 중복 기반의 유사도를 평가합니다.
METRIC = bert_score
